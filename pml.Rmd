---
title: "Course Project"
output: pdf_document
---


# Load stuff
```{r, message=F}
require(caret)
require(ggplot2)
require(reshape2)
require(plyr)

set.seed(42)  # make results reproducible

ds <- read.csv('data/pml-training.csv')
test <- read.csv('data/pml-testing.csv')
```


# Exploratory data analysis

```{r}
ds_complete <- na.omit(ds)

na_count <- apply(ds, 2, function(x) sum(is.na(x)))  # count number of missing values per column
na_cols <- names(na_count[na_count != 0])
complete_cols <- names(na_count[na_count == 0])

na_per_col <- data.frame(na_count)
na_per_col$colname <- rownames(na_per_col)
na_per_col$percent <- (na_per_col$na_count / nrow(ds)) * 100

g <- ggplot(data=na_per_col) +
  geom_histogram( aes(x=percent), bins=40 ) +
  xlab('% NA-values') +
  ylab('number of columns') +
  ggtitle('Percent NA-values in columns') +
  theme(plot.title = element_text(size=17, face='bold', hjust=0.5))
g
```


There are 19622 observations of barbell exercises in the dataset (one observation being one repetition). The number of features is 158. However, the data is not complete. All except 406 observations contain missing values.

The missing values are not distributed uniformly over the columns. 93 columns have no missing values at all. On the other hand, 67 columns have close to 100% of missing values (almost as if they were deleted on purpose... :-) ). With such a high fraction of missing values, one can already suspect that 67 of the features will not be of much use for the prediction. Nevertheless, for the sake of gathering experience, I decided to analyze all features.

```{r, echo=F}
feat <- colnames(ds)
feat <- colnames(ds)[!colnames(ds) %in% c('X', 'classe', 'user_name',
                                                'raw_timestamp_part_1', 'raw_timestamp_part_2', 'cvtd_timestamp',
                                                'new_window', 'num_window')]
```

It has to be noted that the dataset is ordered, with all the class-A observations at the top of the dataset, followed by class-B observations and so on. Therefore, when fitting a preliminary model, the index-variable X - which just indicates the position of the observation in the dataframe - was identified as the most important 'feature'. Of course, in the test-set or a real-word dataset, the data will not be ordered by class. Therefore, I make sure not to include index variable 'X' in the feature list. For the same reason, I excluded the timestamp-columns from the features. The name of the person performing the exercise was also not used as a feature. It is possible that the name is a useful feature for prediction of the test set, because the test set was collected with the same people as the training set. However, the study aims to identify general patterns which do not depend on the person doing the exercise. Therefore, the information who performed the exercise is intentionally ignored in the model.


# Functions for fitting and evaluating random forest models
```{r}
trainRF <- function(trainingdata, features) {
  cv_params <- trainControl(method = 'repeatedcv',
                          number = 4,  # number of folds
                          repeats = 1,  # number of times, the cv is repeated
                          allowParallel = TRUE)
  RFmodel <- train(classe ~ .,
                 data = trainingdata[,c(features, 'classe')],  # only features and outcome variable (no index, etc.)
                 method = 'rf',
                 trControl = cv_params,
                 importance = TRUE,  # for feature importance
                 verbose = FALSE)
  return(RFmodel)
}

get_probabilities <- function(testdata, features, my_model) {
  prob <- predict(my_model,
                  newdata = testdata[,features],
                  type = 'prob')
  prob$id <- rownames(prob)
  long <- melt(prob, id.vars='id')
  return(long[order(long$id),])
}

get_predictions <- function(probabilities) {  # long format dataframe of probabilities
  pred <- ddply(probabilities, 'id', function(x) {  # get prediction from probability
    return(head(subset(x, value == max(x$value)), 1))  # in case of equal probabilites: take first
  })
  colnames(pred) <- c('id', 'prediction', 'probability')
  return(pred)
}

evaluate <- function(predictions, testdata) {  # 
  truth <- data.frame(rownames(testdata), testdata[,'classe'])
  colnames(truth) <- c('id', 'classe')
  compare <- merge(predictions, truth, by='id')
  accuracy <- nrow(subset(compare, prediction == classe)) / nrow(compare)
  return(list(compare, accuracy))
}

get_accuracy <- function(testdata, features, my_model) { # just calculate accuracy (predictions not returned)
  prob <- get_probabilities(testdata, features, my_model)
  pred <- get_predictions(prob)
  return(evaluate(pred, testdata)[[2]])
}
```

# Feature importance

A runtime of about 5 minutes on this small dataset (using 4-fold cross-validation) revealed it wouldn't be computationally feasible to use all observations and all features for fitting a model. Hence, even if the whole dataset was complete, I would have to select a subset of features.

The feature selection process below is very simple. Fist, a preliminary random forest model was fitted using all features and the importance of the features was determined using the varImp-function. Then, a model was fit to the small training set using only the top 5, top 10, top 15, etc. features.

```{r}
# Split the small, complete dataset of 406 rows into a training set and a test set:
inTraining_mini <- createDataPartition(ds_complete$classe, p = .75, list = FALSE)
minitrain <- ds_complete[ inTraining_mini,]
minitest  <- ds_complete[-inTraining_mini,]
```

good link about runtime:
https://www.quora.com/What-is-the-time-complexity-of-Random-Forest-both-building-the-model-and-classification

### Fit preliminary model to analyze feature importance
```{r, message=F}
RFmodel_prelim <- trainRF(minitrain, feat)
```

```{r}
feat_imp <- data.frame(varImp(RFmodel_prelim)$importance)
feat_imp['average'] <- rowMeans(feat_imp[,c('A', 'B', 'C', 'D', 'E')])
feat_imp <- feat_imp[with(feat_imp, order(average, decreasing=TRUE)),]

top50 <- rownames(head(feat_imp, 50))
top30 <- rownames(head(feat_imp, 30))
top20 <- rownames(head(feat_imp, 20))
top10 <- rownames(head(feat_imp, 10))

feat_imp[0:10,'average', drop=FALSE]
```


### Plot accuracy as a function of feature importance

```{r}
get_accuracy_for_feature_subsets <- function(trainingdata, testdata, feature_importance) {
  top_n_features <- (1:10)*5  # which top n features to test
  accuracy <- c()
  for (i in top_n_features) {
    top_features <- rownames(head(feature_importance, i))
    RFmodel <- trainRF(trainingdata, top_features)
    accuracy <- c(accuracy, get_accuracy(testdata, top_features, RFmodel))
  }
  return(data.frame(top_n_features, accuracy))
}
```

```{r}
feat_acc <- get_accuracy_for_feature_subsets(minitrain, minitest, feat_imp)
feat_acc
```

```{r}
g <- ggplot(data=feat_acc, aes(x=top_n_features, y=accuracy)) +
  geom_point(size=3) +
  scale_y_continuous(limits = c(0, 1)) +
  scale_x_continuous(limits = c(1, 50)) +
  xlab('top n features') +
  ggtitle('Accuracy versus number of features') +
  theme(plot.title = element_text(size=17, face='bold', hjust=0.5))
g
```

The plot above shows, that the accuracy increases with the use of more features until about 20 features. Then, the addition of more features does not increase the accuracy. The small differences in accuracy when using 20 or more features represent only the random fluctuations during fitting of a random forest model and can be safely ignored.


# k-nearest neighbour imputation

Due large fraction of missing values in many features, imputation cannot be expected to work very well. Nevertheless, out of curiosity, a k-nearest neigbor imputation model was fitted on the small dataset of complete values (406 rows). Then, using this model, the missing value in the whole dataset (19622 rows) were imputed.

```{r}
knn_imp_model <- preProcess(ds_complete, method = 'knnImpute')
ds_imp <- predict(knn_imp_model, ds)
```


## Split whole dataset into training and validation set

```{r}
inTraining <- createDataPartition(ds$classe, p = .75, list = FALSE)  # split the training data-set

training_imp <- ds_imp[inTraining,]  # training data with imputed values
validation_imp  <- ds_imp[-inTraining,]

training <- ds[inTraining,]  # training data with missing values
validation  <- ds[-inTraining,]
```

## Fit model with imputed features

```{r}
top_imputed_features <- top20

RFmodel_imp <- trainRF(training_imp, top_imputed_features)
prob <- get_probabilities(validation_imp, top_imputed_features, RFmodel_imp)
pred <- get_predictions(prob)
eval <- evaluate(pred, validation_imp)
accuracy <- eval[[2]]
cat('Accuracy of the random forest model trained with the top 20 features (15 with imputed values): ', accuracy)
```

Using the top 20 features - including 15 with mostly imputed values- gives a worse accuracy than just using the best 16 complete features (see next section). The reason for this is mostly the fact that all columns with missing values consist of 99% missing values and only 1% real values. Therefore, the KNN-imputation model is likely to be inaccurate due to a lack of training data. If the missing values were more sparse, it is conceivable that imputation would improve prediction accuracy. In a real-word dataset it would be more likely to find at least a few missing values in every feature and (column) and maybe even a few missing values for each data point (row). In such cases, imputation is not only likely perform better but also necessary because because excluding all data points with missing values would leave only a fraction of the original dataset.

## Fit model with real values only
```{r}
feat_complete <- feat[feat %in% complete_cols]  # features without missing values
top_complete_features <- top50[top50 %in% complete_cols]

RFmodel <- trainRF(training, top_complete_features)
prob <- get_probabilities(validation, top_complete_features, RFmodel)
pred <- get_predictions(prob)
eval <- evaluate(pred, validation)
accuracy <- eval[[2]]
cat('Accuracy of the random forest model trained with the top 16 complete features: ', accuracy)
```

As the prediction was made on a validation set not used in training, the expected accuracy of the random forest model on new data is roughly the same as the accuracy on the validation set.

However, the accuracy on new data could be lower, if the new data is of lower quality (e.g. contains many missing values or is strongly biased). Furthermore, it is important to note that the random element of random forest models leads to random fluctuations of prediction accuracy as well. Even if a new model is built on the same training data, it will have a slightly different prediction accuracy than the old model. To avoid this, a random seed is set at the beginning of the script. This sep ensures that each time a model is trained on the same data, it is exactly the same as the previous one and thus has the same prediction accuracy.

# Predict test-set

```{r}
prob <- get_probabilities(test, top_complete_features, RFmodel)  # 20-row test set
pred <- get_predictions(prob)
cat('Predicions of the 20-row test-set:')
pred
```


# Conclusion

## Accuracy
The prediction accuracy is high even though the random forest model is rather simple could be improved in many ways. A possible explanation for the good results is that the fife ways of performing the barbell exercise are quite different from each other. This is not surprising as the study participants have been instructed to either perform the exercise correctly (class A) or deliberately make a specific mistake (class B to E). A more difficult task would be to have all participants try to do the exercise correctly, have an expert judge how well they do and then use this data to train a model to predict how well the barbell exercise is done. The mistakes would not be as obvious as when made on purpose. In such a realistic setting, prediction accuracy would probably be lower than on this data-set.


